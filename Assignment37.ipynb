{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n\nOverfitting occurs when a model learns the noise and details in the training data, making it too complex and less generalizable to new data. \nConsequences: poor performance on test data, high variance. Mitigation: Use regularization, cross-validation, simpler models, or more data.\n\nUnderfitting happens when the model is too simple and fails to capture the underlying patterns in the data. \nConsequences: poor performance on both training and test data, high bias. Mitigation: Use more complex models, add features, or reduce regularization.\n\n# Q2: How can we reduce overfitting? Explain in brief.\n\nOverfitting can be reduced by:\n Regularization (e.g., L1/L2 penalties).\n Cross-validation to evaluate model performance on unseen data.\n Pruning in decision trees.\n Ensemble methods like bagging and boosting.\n Reducing model complexity (e.g., fewer parameters).\n Increasing training data or using data augmentation.\n    \n# Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n\nUnderfitting occurs when a model is too simple and cannot capture the underlying structure of the data. Scenarios where it can happen:\nUsing a linear model for nonlinear data.\nInsufficient features or too much regularization.\nUsing a model that is too constrained (e.g., a shallow decision tree).\n\n# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n\nBias refers to errors introduced by overly simplistic models (underfitting), leading to inaccurate predictions. \nVariance refers to errors from overly complex models (overfitting), which perform well on training data but poorly on unseen data.\n\nHigh bias leads to underfitting.\nHigh variance leads to overfitting.\nThe tradeoff involves balancing bias and variance to optimize model performance.\n\n# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n\nOverfitting: Detected when the model performs well on training data but poorly on test/validation data. We can use:\nCross-validation performance.\nComparison between training and test errors.\nUnderfitting: Detected when both training and test errors are high. You can:\nCheck the model’s performance on both training and validation data.\n    \n# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n\nHigh Bias: Simple models that make strong assumptions about the data (e.g., linear regression on nonlinear data). Result in underfitting and poor performance.\nHigh Variance: Complex models that are sensitive to fluctuations in the training data (e.g., deep decision trees). Result in overfitting and poor generalization.\n\n# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n\nRegularization adds a penalty term to the model’s loss function to constrain its complexity, preventing overfitting.\nL1 regularization (Lasso): Encourages sparse models by adding the sum of absolute values of coefficients as a penalty.\nL2 regularization (Ridge): Adds the sum of squared coefficients to the penalty, preventing large weights.\nElastic Net: Combines L1 and L2 regularization for better performance in certain cases.\n    ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}